{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8xChkiqlLet"
      },
      "source": [
        "This can be run [run on Google Colab using this link](https://colab.research.google.com/github/CS7150/CS7150-Homework-2/blob/main/HW2_3_CIFAR_classifier.ipynb)\n",
        "# CIFAR-10 Classification (Fully-Connected vs. Convolutional)\n",
        "\n",
        "In this notebook, we will:\n",
        "1. Download **CIFAR-10** (a dataset of 32×32 color images in 10 classes).\n",
        "2. Demonstrate a working classifier using **fully-connected (FC) layers** (a simple MLP).\n",
        "3. **Exercise**: Students will create a **convolutional** version for better efficiency.\n",
        "4. Compare **parameter counts** and performance.\n",
        "\n",
        "This exercise is just an opportunity to understand the power of weight-sharing and play with a standard classification setting that for decades was a focus of machine learning researchers.\n",
        "\n",
        "Try to improve the test performance of the network without making it more expensive to train.  You will just be graded in your experiment findings at the end.\n",
        "\n",
        "**Key Points**:\n",
        "- CIFAR-10 has 60,000 images (50k train, 10k test).\n",
        "- Each image is 3×32×32 (3 color channels).\n",
        "- We’ll flatten those 3×32×32 = 3072 pixels as input to a fully-connected MLP.\n",
        "- Then we’ll invite you to use convolutional layers, which drastically reduce parameters by sharing weights.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_w9x5kRlLeu"
      },
      "source": [
        "## 1. Setup\n",
        "We'll import **PyTorch**, **torchvision**, then load CIFAR-10. We’ll make small transformations (convert to tensors, normalize if desired)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKdeg3nZlLev",
        "outputId": "90e13d7a-cad2-4daa-8bcf-f553fbd84e7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "import numpy as np\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Basic transforms: ToTensor (range [0,1]), optional normalization.\n",
        "transform = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    # Optionally normalize: T.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
        "])\n",
        "\n",
        "# Download and create datasets\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Dataloaders\n",
        "batch_size = 64\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:02<00:00, 81.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhVBc1grlLev"
      },
      "source": [
        "## 2. A Simple Fully-Connected (MLP) Classifier\n",
        "We’ll define a basic MLP:\n",
        "1. Flatten the 3×32×32 image (3072 dims).\n",
        "2. Several **fully connected layers**, then 10 outputs (one per CIFAR-10 class).\n",
        "\n",
        "We can train it for a few epochs—**this won't achieve high accuracy** (CNNs do much better), but it demonstrates the approach."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrzF_nJXlLev",
        "outputId": "ca0b36e5-622a-41b7-9ced-426762ddd5fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self, input_dim=3*32*32, hidden_dim=100, num_classes=10):\n",
        "        super().__init__()\n",
        "        # A small 2-layer MLP:\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
        "    def forward(self, x):\n",
        "        # x: shape (batch, 3, 32, 32)\n",
        "        batch_size = x.size(0)\n",
        "        x = x.view(batch_size, -1)  # flatten\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "mlp = SimpleMLP().to(device)\n",
        "print(\"MLP parameter count:\", sum(p.numel() for p in mlp.parameters() if p.requires_grad))\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP parameter count: 308310\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XcCfMLLlLev"
      },
      "source": [
        "### 2.1 Training Loop\n",
        "We define a simple function `train_epoch` and `test_accuracy` to measure performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoexYXvIlLev"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def train_epoch(model, loader, optimizer, loss_fn=nn.CrossEntropyLoss()):\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    for images, labels in loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(images)\n",
        "        loss = loss_fn(preds, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "def test_accuracy(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            preds = model(images)\n",
        "            predicted = preds.argmax(dim=1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return 100.0 * correct / total\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa5vL1-elLev"
      },
      "source": [
        "Now let's do a short training run on the MLP—**note** that this won't get anywhere close to SOTA accuracy on CIFAR-10, but it demonstrates the pipeline. We'll do maybe **2** or **3** epochs just to see it learns something."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xonWX3m5lLev",
        "outputId": "d4b46b5c-92dd-4d6a-e28c-2911427491df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "mlp = SimpleMLP().to(device)\n",
        "optimizer = optim.Adam(mlp.parameters(), lr=1e-3)\n",
        "\n",
        "epochs = 3  # can increase if you want\n",
        "for epoch in range(1, epochs+1):\n",
        "    train_loss = train_epoch(mlp, train_loader, optimizer)\n",
        "    test_acc = test_accuracy(mlp, test_loader)\n",
        "    print(f\"Epoch {epoch}/{epochs}, train loss={train_loss:.4f}, test acc={test_acc:.2f}%\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3, train loss=1.8878, test acc=35.29%\n",
            "Epoch 2/3, train loss=1.7429, test acc=39.77%\n",
            "Epoch 3/3, train loss=1.6839, test acc=40.44%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZU6VLKrIlLew"
      },
      "source": [
        "## 3. Exercise: Use a Stack of Convolutions\n",
        "\n",
        "CIFAR-10 was **designed** with 2D images in mind, so we can do **far better** with **convolutional** layers that share weights locally.\n",
        "\n",
        "### Your Tasks\n",
        "1. **Construct** a new network (say `ConvNet`) with multiple convolutional layers, optional pooling, etc.\n",
        "2. **Count** the number of parameters. *(Hint: `sum(p.numel() for p in model.parameters() if p.requires_grad)`.)*\n",
        "3. **Train** this model on CIFAR-10. Try to achieve comparable or better accuracy than the MLP **with fewer parameters**.\n",
        "\n",
        "### Suggested Skeleton Code\n",
        "Below is a minimal skeleton. Feel free to modify layer dimensions, add pooling, or add more conv layers. We provide the class structure for you to fill in."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fS5CZ28XlLew"
      },
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(channels)\n",
        "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x  # Preserve input\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += identity  # Add residual\n",
        "        return F.relu(out)\n",
        "\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        # TODO: define your convolutional layers here.\n",
        "        # e.g.\n",
        "        # self.conv1 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, padding=1)\n",
        "        # self.pool = nn.MaxPool2d(2,2)\n",
        "        # etc.\n",
        "        # Then define a final linear layer.\n",
        "        # You have to figure out the shape after the conv layers.\n",
        "\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "\n",
        "        self.res1 = ResidualBlock(32)\n",
        "        self.res2 = ResidualBlock(32)\n",
        "        self.pool1 = nn.MaxPool2d(2,2)  # 32x32 → 16x16\n",
        "\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.res3 = ResidualBlock(64)\n",
        "        self.res4 = ResidualBlock(64)\n",
        "        self.pool2 = nn.MaxPool2d(2,2)  # 16x16 → 8x8\n",
        "\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.res5 = ResidualBlock(128)\n",
        "        self.res6 = ResidualBlock(128)\n",
        "        self.pool3 = nn.MaxPool2d(2,2)  # 8x8 → 4x4\n",
        "\n",
        "        self.fc1 = nn.Linear(128*4*4, 256)\n",
        "        self.fc2 = nn.Linear(256, num_classes)\n",
        "\n",
        "\n",
        "        # self.conv1 = nn.Conv2d(3, 8, kernel_size=3, padding=1)\n",
        "        # self.pool = nn.MaxPool2d(2,2)\n",
        "        # self.conv2 = nn.Conv2d(8, 16, kernel_size=3, padding=1)\n",
        "        # self.pool2 = nn.MaxPool2d(2,2)\n",
        "        # self.conv3 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n",
        "        # self.pool3 = nn.MaxPool2d(2,2)\n",
        "        # # self.conv5 = nn.Conv2d(32, 16, kernel_size=3, padding=1)\n",
        "        # # self.conv6 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
        "        # # self.pool3 = nn.MaxPool2d(2,2)\n",
        "        # # after 2 conv+pool steps, etc...\n",
        "        # # But let's suppose we do only 1 pool, etc.\n",
        "\n",
        "        # self.fc = nn.Linear(16*4*4, 128)  # Just a guess of dimensions.\n",
        "        # self.fc3 = nn.Linear(128, 64)\n",
        "        # self.fc2 = nn.Linear(64, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # # x: (batch, 3, 32, 32)\n",
        "        # x = F.relu(self.conv1(x))  # (batch,8,32,32)\n",
        "        # x = self.pool(x)  # (batch,16,16,16)\n",
        "        # x = F.relu(self.conv2(x))  # (batch,16,16,16)\n",
        "        # x = self.pool2(x)  # (batch,16,16,16)\n",
        "        # x = F.relu(self.conv3(x))  # (batch,16,16,16)\n",
        "        # x = self.pool3(x)  # (batch,16,16,16)\n",
        "        # batch_size = x.size(0)\n",
        "        # x = x.view(batch_size, -1)\n",
        "        # x = self.fc(x)\n",
        "        # x = self.fc3(x)\n",
        "        # x = self.fc2(x)\n",
        "        # return x\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.res1(x)\n",
        "        x = self.res2(x)\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.res3(x)\n",
        "        x = self.res4(x)\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.res5(x)\n",
        "        x = self.res6(x)\n",
        "        x = self.pool3(x)\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLbtVVyNlLew"
      },
      "source": [
        "### 3.1 Code: Train Your ConvNet\n",
        "**Exercise**: Implement the training loop (similar to the MLP), measure test accuracy, and see how you can reduce or increase parameters to trade off accuracy vs. model size.\n",
        "\n",
        "Examples:\n",
        "- Add more conv layers or channels.\n",
        "- Add more or fewer pooling layers.\n",
        "- Print out the param count.\n",
        "- Play with other architectural tricks such as residual connections.\n",
        "- Tweak the learning rate or optimizer.\n",
        "\n",
        "Try to see how low you can go in param count while maintaining a decent accuracy!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9D9VcwBlLew",
        "outputId": "a95215d2-4d66-4ec8-9b56-80b16067aac2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# STUDENT EXERCISE:\n",
        "convnet = ConvNet().to(device)\n",
        "print(\"ConvNet param count:\", sum(p.numel() for p in convnet.parameters() if p.requires_grad))\n",
        "\n",
        "optimizer_conv = optim.Adam(convnet.parameters(), lr=1e-3)\n",
        "epochs_conv = 3\n",
        "for epoch in range(1, epochs_conv+1):\n",
        "    train_loss = train_epoch(convnet, train_loader, optimizer_conv)\n",
        "    test_acc = test_accuracy(convnet, test_loader)\n",
        "    print(f\"[ConvNet] Epoch {epoch}/{epochs_conv}, train loss={train_loss:.4f}, test acc={test_acc:.2f}%\")\n",
        "\n",
        "print(\"\\nNow consider adjusting your ConvNet architecture, parameter count, etc. for better results.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ConvNet param count: 1396746\n",
            "[ConvNet] Epoch 1/3, train loss=1.3916, test acc=63.92%\n",
            "[ConvNet] Epoch 2/3, train loss=0.8237, test acc=68.74%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_AGuN8_lLew"
      },
      "source": [
        "## 4. Report Your Findings\n",
        "\n",
        "Points to understand:\n",
        "\n",
        "1. A **fully-connected** approach to image classification (such as CIFAR-10) can work but tends to have **many** parameters (e.g., 3,072×100 just in one layer on tiny images) and typically yields lower accuracy compared to modern **Convolutional** architectures.\n",
        "2. **Convolution** drastically reduces parameter counts via **weight sharing**, can often achieve much higher accuracy on image tasks, and is typically *translation-equivariant*.\n",
        "3. Your goal is to **experiment** with different conv net designs to minimize param count while maximizing accuracy.\n",
        "\n",
        "Report here at least two iterations of your architectural experiments:\n",
        "\n",
        "1. Using an architecture consisting of $\\fbox{your answer}$, I was able to reduce the parameterization to $\\fbox{your answer}$ parameters and achieve test accuracy of $\\fbox{your answer}$ after three epochs of training.\n",
        "\n",
        "2. In a second test, I tried an architecture consisting of $\\fbox{your answer}$.  That used an even smaller parameterization, with only $\\fbox{your answer}$ parameters, and it achieved test accuracy of $\\fbox{your answer}$ after three epochs of training.\n",
        "\n",
        "Good luck!"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GgMFRc9VID3r"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "name": "CIFAR10_Classifier_FC_vs_Conv",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "mimetype": "text/x-python",
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}